**王伟营 202128014628067 作业3**



## 试析随机森林为何比决策树 Bagging 集成的训练速度更快。

Bagging在选择划分属性时需要考察结点的所有属性，而随机森林选择划分属性时需要考察结点的所有属性，而随机森林只需随机地考察一个属性子集，计算复杂度更低，所以随机森林的训练效率常优于Bagging。

##  试比较 Gradient Boosting 与 AdaBoost 的异同。

 AdaBoost算法针对指数型损失函数，对于一般的损失函数不适用；Gradient Boosting适合任意形式的损失函数；

## 试比较包裹式选择、过滤式选择与嵌入式选择的异同。

包裹式特征选择方法：特征选择与分类相结合，特征评价判据为分类器性能。计算量大。

过滤式特征选择方法：特征选择和分类单独进行，特征评价判据间接反映分类性能，计算量小。

嵌入式选择：将分类器学习和特征选择融为一体，分类器训练过程自动完成了特征选择。

## 试述直接求解 L0 范数正则化会遇到的困难。

L0范数是非0范数的个数，L0范数是非凸且离散的，没法求导。

## 试述为什么基于 L1 范数可以进行特征选择。

最小化L1范数可得到稀疏解，切线与切点的交点在坐标轴上，解向量的大部分位置值为零，只有少部分位置值不为零。

## 试比较 K-SVD 与 K-means 方法的异同。

K-means通过最近邻来寻找数据点的最优表示字典，字典由均值构成，数据点的稀疏编码仅有一个非零值。K-means是字典学习的一个特例。

K-SVD字典不一定是均值，可以自己学出来的一般的字典，数据点的稀疏编码有多个非零值。

