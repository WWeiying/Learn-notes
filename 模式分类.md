# 模式分类

## 第二章 贝叶斯决策论

### 引言

- 类条件概率密度/状态条件概率密度：

	- 类别状态为w时的x的概率密度函数

- 先验概率

$$
P(w_j)
$$
- 贝叶斯公式：

	- 

$$
posterior=\frac{likelihood\times prior}{evidence}
$$
	- 

$$
\begin{split}&通过观测x的值将先验概率P(w_j)转换成后验概率P(w_j|x)，\\&即假设特征值x已知的条件下类别属于w_i的概率。\end{split}
$$
	- 

$$
\begin{align} &p(x|w_j)是w_j关于x的似然函数/似然(likelihood),\\&表明在其他条件都相等的情况下，使得p(x|w_j)的w_j更有可能是真实的类别。\\
\end{align}
$$
	- 

$$
\begin{align}&证据因子p(x)仅是一个标量因子，表明实际观测的具有特征值x的模式的\\&出现频率，保证各类别的后验概率总和为1，从而满足概率条件。\end{align}
$$

- 某次判决时的误差概率

	- 平均误差概率

$$
P(error)=\int_{-\infty}^{\infty} P(error,x)\, dx=\int_{-\infty}^{\infty} P(error|x)p(x)\, dx
$$

- 最小化误差概率条件下的贝叶斯决策规则

$$
\begin{align}&如果P(w_1|x)>P(w_2|x),判别为w_1;否则判别为w_2\\&\Longleftrightarrow 如果p(x|w_1)P(w_1)>p(x|w_2)P(w_2),判别为w_1;否则判别为w_2\end{align}
$$

### 贝叶斯决策论——连续特征

- 推广

	- 允许使用多于一个特征
	- 允许多于两种类别状态的情形
	- 允许有其他行为而不是仅仅判定类别

		- 允许存在拒绝决策的可能性

	- 通过引入一个更一般的损失函数来替代误差概率

- 

$$
\begin{align}&风险函数\lambda(\alpha_i|w_j)描述类别状态\\&为w_j时采取行动\alpha_i的风险\end{align}
$$

	- 

$$
\begin{align}&P(w_j|x)是实际类别状态为w_j时的概率，与行为\alpha_i相关的损失：\\&条件风险R(\alpha_i|x)=\sum_{j=1}^c\lambda(\alpha_i|w_j)P(w_j|x)\end{align}
$$
	- 

$$
\begin{align}&选择行为\alpha_i使R(\alpha_i|x)最小化， 最小化的总风险值\\&称为贝叶斯风险，记为R^*，
它是可获得的最优的结果。\end{align}
$$

- 两类分类问题

	- 

$$
\begin{align}&条件风险：\\&R(\alpha_1|x)=\lambda_{11}P(w_1|x)+\lambda_{12}P(w_2|x)\quad \\&R(\alpha_2|x)=\lambda_{21}P(w_1|x)+\lambda_{22}P(w_2|x)\end{align}
$$
	- 

$$
\begin{align} &最小风险决策规则：R(\alpha_1|x)<R(\alpha_2|x),\\&则判为w_1。\\
&\Longrightarrow 用后验概率的形式表述为：\\
&如果(\lambda_{21}-\lambda_{11})P(w_1|x)>(\lambda_{12}-\lambda_{22})P(w_2|x),\\&那么判决为w_1。\end{align}
$$

		- 

$$
\begin{align}&一次错误判决造成的损失比正确判决大。\\ &所以\lambda_{21}-\lambda_{11}>0, \lambda_{12}-\lambda_{22}>0\end{align}
$$

	- 

$$
\begin{align} \Longrightarrow 
&等价规则用先验概率和条件概率的形式表述为：\\
&如果(\lambda_{21}-\lambda_{11})p(x|w_1)P(w_1)>(\lambda_{12}-\lambda_{22})p(x|w_2)P(w_2),\\&那么判决为w_1，否则判决为w_2。\end{align}
$$
	- 

$$
\Longrightarrow \frac{p(x|w_1)}{p(x|w_2)}>\frac{\lambda_{12}-\lambda_{22}}{\lambda_{21}-\lambda_{11}}\frac{P(w_2)}{P(w_1)}
$$

		- 

$$
\begin{align}&贝叶斯规则可以解释为\\&如果似然比超过某个不依赖观测值x的阈值，则可判决为w_1。\end{align}
$$

### 最小误差率分类

- 对称损失/0-1损失函数

$$
\lambda(\alpha_i|w_j)= \begin{cases} 
0&i=j\\
1&i\neq j
\end{cases}\quad i, j=1,...,c
$$

	- 所有误判都是等代价的

- 条件风险

$$
\begin{align} R(\alpha|x)&=\sum_{j=1}^c\lambda(\alpha_i|w_j)P(w_j|x)\\&=\sum_{j\neq i}P(w_j|x)=1-P(w_i|x)
\end{align}
$$

	- 

$$
\begin{align} &贝叶斯规则要求选择（最小化平均误差概率）一种能使条件\\&风险最小化的行为，既是选取i使得后验概率P(w_i|x)最大。\\
&\Longrightarrow \forall j\neq i,如果P(w_i|x)>P(w_j|x)，则判决为w_i。
\end{align}
$$

- 极小化极大准则

	- 使先验概率取任意值引起的总风险的最坏情况尽可能小，即最小化最大可能的总风险。

- Neyman-Pearson准则

### 分类器、判别函数及判定面

- 多类情况/多重分类器

	- 

$$
\begin{align} &最大判别函数与最小条件风险对应：\\
&g_i(x)=-R(\alpha_i|x)\\
&最大判别函数与最大后验概率对应：\\
&g_i(x)=P(w_i|x)\\
\end{align}
$$
	- 

$$
\begin{align} &判别函数的选择不是唯一的，简单代表如下：\\
&g_i(x)=P(w_i|x)=\frac{p(x|w_i)P(w_i)}{\sum_{j=1}^c p(x|w_j)P(w_j)}\\
&g_i(x)=p(x|w_i)P(w_i)\\
&g_i(x)=lnp(x|w_i)+lnP(w_i)\\
\end{align}
$$
	- 

$$
\begin{align} &判决规则唯一：\\
&每种判决规则都是将特征空间分成c个判决区域，\\
& R_1,...,R_c，如果对于所有j\neq i有g_i(x)>g_j(x)，\\
&那么x属于R_i，判决规则要求我们将x分给w_i。\\
&此区域由判别边界来分割，判别边界是判决空间\\
&中使判别函数值最大的曲面。\end{align}
$$

- 两类情况/二分分类器

	- 

$$
\begin{align} &简单的判别函数：\\
&g(x)\equiv g_1(x)-g_2(x)\\
&判决规则：\\
&如果g(x)>0，则判为w_1；否则判为w_2。\\
\end{align}
$$
	- 

$$
\begin{align} &g(x)=P(w_1|x)-P(w_2|x)\\
&g(x)=ln\frac{p(x|w_1)}{p(x|w_2)}+ln\frac{P(w_1)}{P(w_2)}\\
\end{align}
$$

### 正态密度

- 单变量高斯密度函数

	- 

$$
\begin{align} &连续分布函数的数学期望：\\
&\varepsilon[f(x)]=\int_{-\infty}^{\infty} f(x)p(x)\, dx\\
&离散分布函数的数学期望：\\
&\varepsilon[f(x)]=\sum_{x\in D}f(x)P(x)\\
\end{align}
$$
	- 

$$
\begin{align} &单变量高斯密度函数：\\
&p(x)=\frac1{\sqrt{2\pi}}e^{-\frac12(\frac{x-\mu}{\sigma})^2}\\
&期望：\\
&\mu=\varepsilon[x]=\int_{-\infty}^{\infty}xp(x)\,dx\\
&方差：\\
&\sigma^2=\varepsilon[(x-\mu)^2]=\int_{-\infty}^{\infty}(x-\mu)^2p(x)\,dx\\
\end{align}
$$

		- 正态分布在所有给定的均值和方差的分布中具有最大熵。大量小的、独立的随机分布的和为高斯分布——中心极限定理。
		- 

$$
p(x)\sim N(\mu,\sigma^2)
$$

- 多元高斯密度函数

	- 

$$
\begin{align} &d维多元正态密度：\\
&p(x)=\frac1{(2\pi)^{d/2}|\Sigma|^{1/2}}e^{-\frac12(x-\mu)^T\Sigma^{-1}(x-\mu)}\\
&x是d维列向量，\mu是d维均值向量\\
&\Sigma是d维协方差矩阵
\end{align}
$$

		- 

$$
p(x)\sim N(\mu,\Sigma)
$$

	- 

$$
\begin{align}&\mu=\varepsilon[x]=\int xp(x)\, dx\\ &\Sigma=\varepsilon[(x-\mu)(x-\mu)^T]=\int(x-\mu)(x-\mu)^Tp(x)\, dx\\
&其中某个向量或矩阵的均值通过其元素求得:\\
&\mu_i=\varepsilon[x_i]\\
&\sigma_{ij}=\varepsilon[(x_i-\mu_i)(x_j-\mu_j)]\\
&协方差矩阵\Sigma通常是对称的且半正定的。对角线元素\sigma_{ii}是x_i的方差,\\
&非对角线元素\sigma_{ij}是x_i和x_j的协方差。\\
&随机变量x_i、x_j相互独立，则\sigma_{ij}=0。
\end{align}
$$
	- 

$$
\begin{align} &服从正态分布的随机变量的线性组合，不管随机变量独立与否，\\&也是正态分布。\\
&p(x)\sim N(\mu,\Sigma)，y=A^Tx—k维向量，A-d\times k维矩阵，\\
&则p(y)\sim N(A^T\mu,A^T\Sigma A)。\\
&例：k=1, A是一单位向量a，y=a^Tx是一标量，表示x\\
&到沿a方向的一条直线的投影。a^T\Sigma a是x向a投影的方差。\\
&白化变换：A_w=\Phi\Lambda^{-1/2}使得变换后分布的协方差矩阵变成\\
&单位阵，产生一个圆周对称的高斯分布，将任意的多元正态分布\\
&转换到一个球坐标系。\\
&矩阵\Phi,其列向量是\Sigma的特征向量,\Lambda是本征值对应的对角矩阵。\\
\end{align}
$$
	- 

$$
\begin{align} &等密度的点的轨迹为一超椭球体，其主轴由\Sigma的本征向量（由\Phi表示）,\\
&本征值（由\Lambda表示）决定主轴的长度。\\
&r^2=(x-\mu)^T\Sigma^{-1}(x-\mu)\\
&称为从x到\mu的平方Mahalanobis距离（马氏距离），等密度的边界是\\
&一些到\mu的恒定马氏距离的超椭球体，且其体积决定了均值附近样本的离散\\
&程度，与马氏距离r对应的超椭球体体积为：\\
&V=V_d|\Sigma|^{1/2}r^d, v_d是d维单位超椭球体的体积：\\
&V_d=\begin{cases}
\pi^{d/2}/(d/2)!&d为偶数\\
2^d\pi^{(d-1)/2}(\frac{d-1}2)!&d为奇数\\
\end{cases}\\
&对于一给定维数，样本的离散程度随|\Sigma|^{1/2}而变化。\\
\end{align}
$$

- 正态分布的判别函数

	- 

$$
\begin{align} &最小化误差概率的判别函数：g_i(x)=lnp(x|w_i)+lnP(w_i)\\
&密度函数p(x|w_i)服从多元正态分布，即p(x|w_i)\sim N(\mu_i,\Sigma_i)。\\
&则g_i(x)=-\frac12(x-\mu_i)^T\Sigma_i^{-1}(x-\mu_i)-\frac d2ln2\pi-\frac12ln|\Sigma_i|+lnP(w_i)
\end{align}
$$
	- 

$$
\begin{align} &情况1：\Sigma_i=\sigma^2I\\
&发生在各特征统计独立，并且每个特征具有相同的方差\sigma^2时,此时\\
&协方差矩阵是对角阵，仅仅是\sigma^2与单位阵I的乘积。\\
&几何上，样本落于相等大小的超球体聚类中，第i类的聚类以均值向量\mu_i为中心。\\
&|\Sigma_i|=\sigma^{2d}\quad \Sigma_i^{-1}=(1/\sigma^2)/I\\
&简化后的判别函数：\\
&g_i(x)=-\frac{||x-\mu_i||^2}{2\sigma^2}+lnP(w_i)=-\frac12[x^Tx-2\mu_i^Tx+\mu_i^T\mu_i]+lnP(w_i)\\
&\Longrightarrow g_i(x)=w_i^Tx+w_{i0}\\
&其中w_i=\frac1{\sigma^2}\mu_i, w_{i0}=\frac{-1}{2\sigma^2}\mu_i^T\mu_i+lnP(w_i)称为第i个方向的阈值/偏置\\
&线性机器的判定面是一个超平面，若P(w_i)=P(w_j),则超平面垂直平分\\
&两中心的连线；若P(w_i)\neq P(w_j),则超平面将远离可能的均值。\\
\end{align}
$$
	- 

$$
\begin{align} &情况2：\Sigma_i=\Sigma\\
&所有类的协方差矩阵都相等，但各自的均值向量是任意的。\\
&样本落在相同大小、形状的超椭球体聚类中，第i类聚类中心在向量\mu_i附近。\\
&\Longrightarrow g_i(x)=-\frac12(x-\mu_i)^T\Sigma^{-1}(x-\mu_i)+lnP(w_i)\\
&最优判决规则：将x归于离它最近的均值所属的类，不相等的先验概率将\\
&判定面移向远离先验概率较大的类的一边。\\
&\Longrightarrow g_i(x)=w_i^Tx+w_{i0},其中w_i=\Sigma^{-1}\mu_i,w_{i0}=-\frac12\mu_i^T\Sigma^{-1}\mu_i+lnP(w_i)\\
&判别函数是线性的，判决边界同样是超平面，通常超平面不与均值连线垂直相交。\\
\end{align}
$$
	- 

$$
\begin{align} &情况3：\Sigma_i=任意\\
&一般的多元正态分布的情况下，每一类的协方差矩阵是不同的。判别函数如下：\\
&g_i(x)=x^TW_ix+w_i^Tx+w_{i0},其中W_i=-\frac12\Sigma_i^{-1},w_i=\Sigma_i^{-1}\mu_i\\
&w_{i0}=-\frac12\mu_i^T\Sigma_i^{-1}\mu_i-\frac12ln|\Sigma_i|+lnP(w_i)\\
&在两类问题中，判定面为超二次曲面。
\end{align}
$$

### 贝叶斯决策论——离散特征

- 

$$
\begin{align} &贝叶斯公式的概率密度函数p(.)变成概率分布函数P(.)。\\
&条件风险R(\alpha|x)的定义不变，且贝叶斯决策论的基础不变：\\
&为减少总风险，选择行为\alpha_i使R(\alpha_i|x):\alpha^*=arg\min_iR(\alpha_i|x)\\
&通过最大化后验概率来最小化误差率的基本原则也不变。\\
\end{align}
$$
- 独立的二值特征

	- 

$$
\begin{align} &考虑两类问题,特征向量的元素是二值的，并且条件独立，\\
&x=(x_1,...,x_d)^T,x_i=0/1,\\
&且p_i=Pr[x_i=1|w_1]，q_i=Pr[x_i=1|w_2]\\
&每一特征量给出关于该模式的“是”“否”的答案\\
&类条件概率：\\
&P(x|w_1)=\prod_{i=1}^dp_i^{x_i}(1-p_i)^{1-x_i}\\
&P(x|w_2)=\prod_{i=1}^dq_i^{x_i}(1-q_i)^{1-x_i}\\
&似然比：\\
&\frac{P(x|w_1)}{P(x|w_2)}=\prod_{i=1}^d(\frac{p_i}{q_i})^{x_i}(\frac{1-p_i}{1-q_i})^{1-x_i}\\
\end{align}
$$

		- 

$$
\begin{align} &判决函数：\\
&g(x)=\Sigma{i=1}^d[x_iln\frac{p_i}{q_i}+(1-x_i)ln\frac{1-p_i}{1-q_i}]+ln\frac{P(w_1)}{P(w_2)}\\
&判决函数对x_i是线性的—g(x)=\sum_{i=1}^dw_ix_i+w_0\\
&w_i=ln\frac{p_i(1-q_i)}{q_i(1-p_i)}\quad i=1,...,d\\
&w_0=\sum_{i=1}^dln\frac{1-p_i}{1-q_i}+ln\frac{P(w_1)}{P(w_2)}\\
&若g(x)>0,则判为w_1;若g(x)\leq 0,则判为w_2。\\
&g(x)是x的各分量的加权组合；权重w_i的幅值表示进行分\\
&类时，x_i与一个是的回答相关联的程度。\\
&若p_i=q_i,没有给出有关类别的信息，则w_i=0;\\
&若p_i>q_i,w_i>0,则权值贡献给判决w_1。\\
&若p_i<q_i,w_i<0,则权值贡献给判决w_2。\\
\end{align}
$$

			- 

$$
\begin{align} &特征独立的条件产生简单的线性分类器，特征不独立\\
&产生更加复杂的分类器。先验概率P(w_i)通过阈值权\\
&系数w_0的形式出现在判决函数中，增大P(w_1)即增\\
&大w_0且将判决偏向w_1类，减小P(w_1)则相反。\\
\end{align}
$$

## 第三章 最大似然估计和贝叶斯估计

### 引言

- 最大似然估计将待估计的参数看作确定性的量，只是其取值未知。最佳估计是使得产生已观测的样本（即训练样本）的概率为最大的那个值。
- 参数估计把待估计参数看成是符合某种先验概率分布的随机变量。对样本进行观测的过程，就是把先验概率密度转化成后验概率密度，这样就利用样本的信息修正了对参数的初始估计值。

	- 在贝叶斯估计中，一个典型的效果就是，每得到新的观测样本，都使得后验概率密度函数变得更加尖锐，使其在待估参数的真实值附近形成最大的尖峰。

- 有监督学习/无监督学习

	- 相同点：产生某个样本x的过程
	- 不同点：有监督学习的每一个样本的自然状态是已知的，即样本的标记是已知的；无监督学习的每个样本的自然状态是未知的。

- 非参数化方法

	- 通常先对特征空间进行变换，然后在变换空间中再采用参数化的方法，用以达到简化问题的目的。

### 最大似然估计

- 优秀的性质：在样本增多时通常收敛得非常好；通常比其他方法要简单，很适合实际应用。
- 基本原理

	- 

$$
\begin{align} &已知样本集D，其中每一个样本都是独立的根据已知形式的\\
&概率密度函数p(x|\theta)抽取得到的，要求使用这些样本，估计\\
&概率密度函数中的参数向量的值。假设样本集D中有n个样\\
&本：x_1,x_2,...,x_n已知，把p(D|\theta)看成参数向量\theta的函数，\\
&称为样本集D下的似然函数。p(D|\theta)=\prod_{k=1}^np(x_k|\theta)
\end{align}
$$
	- 

$$
\begin{align} &参数向量\theta的最大似然估计，就是使p(D|\theta)达到最大值\\
&的那个参数向量\hat{\theta}。直观的理解：参数向量\theta的最大似然\\
&估计就是最符合已有观测样本集的那一个。
\end{align}
$$
	- 

$$
\begin{align} &待求参数的个数为p。则参数向量\theta写成p维向量形式：\\
&\theta=(\theta_1,\theta_2,...,\theta_p)^t。记\nabla_\theta为如下的梯度算子：\nabla_\theta=\begin{bmatrix}
\frac\partial{\theta_1}\\
.\\
.\\
.\\
\frac\partial{\theta_p} \\
\end{bmatrix}\\
\end{align}
$$

		- 

$$
\begin{align} &定义对数似然函数l(\theta)=lnp(D|\theta),求解使对数似然函数最\\
&大的\theta的规范形式：\hat{\theta}=arg\max_\theta l(\theta),l(\theta)=\sum_{k=1}^nlnp(x_k|\theta)\\
&\nabla_\theta l=\sum_{k=1}^n\nabla_\theta lnp(x_k|\theta),\\
&求解最大似然估计值\theta的必要条件，由p个方程组成的方程组\\
&\nabla_\theta l=0。
\end{align}
$$

	- 最大后验（maximum a posteriori, MAP）估计器
	
		- 

$$
\begin{align} &求使l(\theta)p(\theta)取最大值的参数向量\theta, p(\theta)是对参数向量\theta\\
&取不同值的概率的先验估计。最大似然估计器是先验概率p(\theta)\\
&为均匀分布时的MAP估计器。其缺点是，对参数空间作非线\\
&性变换，概率密度p(\theta)会发生变化，MAP估计结果不再有效。\\
\end{align}
$$

- 

$$
高斯情况：\mu未知
$$

	- 

$$
\begin{align} &考虑一个训练样本点x_k，有下列式子成立：\\
&\ln p(x_k|\mu)=-\frac12\ln[(2\pi)^d|\Sigma|]-\frac12(x_k-\mu)^T\Sigma^{-1}(x_k-\mu)\\
&\nabla_\mu\ln p(x_k|\mu)=\Sigma^{-1}(x_k-\mu) \\
&对\mu的最大似然估计值必须满足下式：\\
&\sum_{k=1}^n\Sigma^{-1}(x_k-\hat{\mu})=0,两边同乘以协方差矩阵得：\\
&\hat{\mu}=\frac1n\sum_{k=1}^nx_k\\
&这个公式说明：对均值的最大似然估计就是对全体样本取\\
&平均,即实际均值的最大似然估计等于样本均值。\\
\end{align}
$$

- 

$$
高斯情况：\mu和\Sigma均未知
$$

	- 

$$
\begin{align} &参数向量\theta的组成成分是：\theta_1=\mu,\theta_2=\sigma^2。单个训练样本的\\
&对数似然函数为：\\
&\ln p(x_k|\theta)=-\frac12\ln 2\pi\theta_2-\frac1{2\theta_2}(x_k-\theta_1)^2\\
&对变量\theta求导:\\
&\nabla_\theta l=\nabla_\theta\ln p(x_k|\theta)=
\begin{bmatrix}
\frac1{\theta_2}(x_k-\theta_1)\\
-\frac1{2\theta_2}+\frac{(x_k-\theta_1)^2}{2\theta_2^2}\end{bmatrix}\\
&得到对全体样本的对数似然函数的极值条件：\\
&\sum_{k=1}^n\frac1{\hat{\theta}_2}(x_k-\hat{\theta}_1)和-\sum_{k=1}^n\frac1{\hat{\theta}_2}+\sum_{k=1}^n\frac{(x_k-\hat{\theta}_1)^2}{\hat{\theta}_2^2}=0\\
&其中\hat{\theta_1}，\hat{\theta_2}分别为\theta_1,\theta_2的最大似然估计。
\end{align}
$$
	- 

$$
\begin{align} &把\hat{\theta}_1，\hat{\theta}_2用\hat{\mu},\hat{\sigma}^2代替，得到均值、方差的最大似然估计：\\
&\hat{\mu}=\frac1n\sum_{k=1}^nx_k\\
&\hat{\sigma}^2=\frac1n\sum_{k=1}^n(x_k-\hat{\mu})^2\\
&多元高斯分布的均值\mu和协方差矩阵\Sigma的最大似然估计为：\\
&\hat{\mu}=\frac1n\sum_{k=1}^nx_k\\
&\hat{\Sigma}=\frac1n\sum_{k=1}^n(x_k-\hat{\mu})(x_k-\hat{\mu})^T\\
&实际均值的最大似然估计是样本均值，协方差的最大似然估计是n个(x_k-\hat{\mu})(x_k-\hat{\mu})^T矩阵的算数平均。\\
\end{align}
$$

- 估计的偏差

	- 

$$
\begin{align} &对方差\sigma^2的最大似然估计是有偏估计：\varepsilon[\frac1n\sum_{i=1}^n(x_i-\overline{x})^2]=\frac{n-1}n\sigma^2\neq \sigma^2\\
&对协方差矩阵的最大似然估计\hat{\Sigma}=[(n-1)/n]C是有偏估计，是渐进无偏估计。\\
&对协方差矩阵的无偏估计:样本协方差矩阵C=\frac1{n-1}\sum_{k=1}^n(x_k-\hat{\mu})(x_k-\hat{\mu})^T\\
&如果一个估计器对所有分布都是无偏的，那么它称为绝对无偏的。\\
&如果一个估计器在样本数n很大时趋于无偏估计，那么它称为渐进无偏的。\\
\end{align}
$$

### 贝叶斯估计

- 

$$
\begin{align} &贝叶斯学习的核心问题：已知一组训练样本D，这些样本都是从固定\\
&但未知的概率密度函数p(x)中独立抽取的，要求根据这些样本估计\\
&p(x|D)，贝叶斯公式为：P(w_j|x,D)=\frac{p(x|w_i,D_i)P(w_i)}{\sum_{j=1}^c p(x|w_j,D_j)P(w_j)}
\end{align}
$$

## 第四章 非参数方法

### 引言

非参数方法能处理任意的概率分布，不必假设条件概率密度函数的参数形式已知。

非参数方法：

* 如何从训练样本中估计条件概率密度函数$p(\bold{x}|w_j)$
* 如何直接估计后验概率$P(w_j|\bold{x})$

### $k_n$近邻估计



