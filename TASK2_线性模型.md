# 第3章 线性模型

##### 基本形式

线性模型linear model的向量形式：$f(x)=w^Tx+b,w=(w_1;w_2,...,w_d)$

优点：形式简单，易于建模，w直观表达了各属性在预测中的重要性，因此模型有很好的可解释性。非线性模型可以通过映射线性模型得到。

##### 一元线性回归

线性回归试图学得一个线性模型以尽可能准确地预测实值输出标记。

离散属性：

若属性值存在序关系，通过连续化转化为连续值；

若属性值不存在序关系，有k个属性值，转化为k维向量。

均方误差：

$E_{(w,b)}=\sum_{i=1}^m(y_i-wx_i-b)^2$

$(w^*,b^*)=arg \min_{(w,b)}\sum_{i=1}^m(f(x_i)-y_i)^2=arg \min_{(w,b)}\sum_{i=1}^m(y_i-wx_i-b)^2$

基于均方误差最小化的模型求解方法叫做最小二乘法。

线性回归中，最小二乘法试图找到一条直线使所有样本到直线的欧氏距离之和最小。

求解w和b使均方误差最小化的过程叫做线性回归的最小二乘参数估计。

极大似然估计估计概率分布的参数值，直观想法是使得观测样本出现概率最大的分布就是待求，即使得联合概率（似然函数）$L(\theta)$取得最大值的$\theta^*$即为$\theta$的估计值。

极大似然估计与最小二乘估计殊途同归。

多元函数的一阶导数称为函数$f(x)$在$x$处的一阶导数或梯度$\nabla f(x)$。

多元函数的二阶导数称为函数$f(x)$在$x$处的二阶导数或Hessian（海塞）矩阵$\nabla^2f(x)$。

证明一个多元函数的海塞矩阵是半正定的，即可证明该函数是凸函数。

凸充分性定理：

若$f:R^n \longrightarrow R$是凸函数，且$f(x)$一阶连续可微，则$x^*$是全局解的充分必要条件是$\nabla f(x)=0$。

##### 多元线性回归

样本由d个属性描述

$f(x_i)=w^Tx_i+b,使得f(x_i)\cong y_i$

$f(\hat{x}_i)=\hat{w}^T\hat{x}_i,\hat{w}^T=(w_1,w_2,...,w_d,w_{d+1}),\hat{x_i}=(x_{i1},x_{i2},...,x_{id},1)^T$

$\hat{w}^*=arg \min_{\hat{w}}(y-X\hat{w})^T(y-X\hat{w})$

求得的线性回归模型为：

$f(\hat{x_i})=\hat x_i^T(X^TX)^{-1}X^Ty$

广义线性模型：

$g(.)$是单调可微函数，连续且充分光滑，$y=g^{-1}(w^Tx+b)$

##### 对数几率回归（实际上是分类）

在线性模型的基础上套一个映射函数来实现分类功能。

对数几率函数（sigmoid函数）：

$y=\frac1{1+e^{-(w^Tx+b)}}$

用极大似然法估计w和b，用最优化方法如梯度下降法、牛顿法求其最优解。

信息论与最大似然估计殊途同归。

##### 线性判别分析

略





















